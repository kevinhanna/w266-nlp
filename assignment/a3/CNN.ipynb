{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "This notebook introduces convolutional neural networks (CNNs), a more powerful classification model similar to the Neural Bag-of-Words (BOW) model you explored earlier.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Part (a):** Model Architecture\n",
    "- **Part (b):** Implementing the CNN Model\n",
    "- **Part (c):** Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from w266_common import patched_numpy_io\n",
    "# Code for this assignment\n",
    "import sst\n",
    "\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Model Architecture\n",
    "\n",
    "CNNs are a more sophisticated neural model for sentence classification than the Neural BOW model we saw in the last section. CNNs operate by sweeping a collection of filters over a text. Each filter produces a sequence of feature values known as a _feature map_. In one of the most basic formulations introduced by [Kim (2014)](https://www.aclweb.org/anthology/D14-1181), a single layer of _pooling_ is used to summarize the _feature maps_ as a fixed length vector. The fixed length vector is then feed to the output layer in order to produce classification labels. A popular choice for the pooling operation is to take the maximum feature value from by each _feature map_.\n",
    "\n",
    "![Convolutional Neural Network from Kim 2014](kim_2014_figure_1_cnn.png)\n",
    "*CNN model architure, Figure 1 from Kim (2014)*\n",
    "\n",
    "We'll use the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$, the word id for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x^{i:i+j}$ is the concatenation of $x^{(i)}, x^{(i+1)} ... x^{(i+j)}$ \n",
    "- $c^{(i)}_{k}$ is the value of the $k^{th}$ feature map along the word sequence, each filter applies over a window of $h$ words and uses non-linearity $f$.\n",
    "- $\\hat{c}_{k}$ is the value of the $k^{th}$ feature after pooling the feature map over the whole sequence.\n",
    "- $\\hat{C}$ is the concatenation of pooled feature maps. \n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Convolutional layer:** $c^{(i)}_{k} = f(x^{i:i+h-1} W_k + b)$\n",
    "- **Pooling layer:**  $\\hat{c}_{k}$ = $max(c^{(0)}_{k}, c^{(1)}_{k}...)$ \n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(\\hat{C} W_{out} + b_{out})$\n",
    "\n",
    "\n",
    "We'll refer to the first part of this model (**Embedding layer**, **Convolutional layer**, and **Pooling layer**) as the **Encoder**: it has the role of encoding the input sequence into a fixed-length vector representation that we pass to the output layer.\n",
    "\n",
    "We'll also use these as shorthand for important dimensions:\n",
    "- `V`: the vocabulary size (equal to `ds.vocab.size`)\n",
    "- `N`: the maximum number of tokens in the input text\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `kernel_size`: a list of filter lengths\n",
    "- `filters`: number filters per filter length\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Short Answer Questions\n",
    "\n",
    "When answering these questions in the answers file,\n",
    "`embed_dim = 10`, `kernel_size = [3, 4, 5]`, `filters=128`, `N=10` and `num_classes = 7`.\n",
    "\n",
    "1. In terms of these values, the vocabulary size `V` and the maximum sequence length `N`, what are the\n",
    "   shapes of the following variables: \n",
    "   $c^{(i)}_{kernal\\_size=3}$, $c^{(i)}_{kernal\\_size=4}$, $c^{(i)}_{kernal\\_size=5}$, $\\hat{c}^{(i)}_{kernal\\_size=3}$, $\\hat{c}^{(i)}_{kernal\\_size=4}$, $\\hat{c}^{(i)}_{kernal\\_size=5}$, and $\\hat{C}$. Assume a stride size of 1. Assume padding is not used (e.g., for tf.nn.max_pool and tf.nn.conv1d, setting padding='VALID'), provide the shapes listed above.\n",
    "<p>\n",
    "2. What are the shapes of $c^{(i)}_{kernal\\_size=3}$ and $\\hat{c}^{(i)}_{kernal\\_size=3}$ when paddiding is used.\n",
    "      (e.g., for tf.nn.max_pool and tf.nn.conv1d, setting padding='same').\n",
    "<p>\n",
    "3. How many parameters are in each of the convolutional filters, $W_{filter\\_length=3}$, $W_{filter\\_length=4}$, $W_{filter\\_length=5}$? And the output layer, $W_{out}$?\n",
    "<p>\n",
    "<p>\n",
    "4. Historically NLP models made heavy use of manual feature engineering. In relation to systems with manually engineered features, describe what type of operation is being performed by the convolutional filters.\n",
    "<p>\n",
    "5. Suppose that we have two examples, `[foo bar baz]` and `[baz bar foo]`. Will this model make the same predictions on these? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-63ce3f091862>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-63ce3f091862>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    convolutional_neural_networks_a_4: your answer\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Question 1.1 (/2): What is the dimension of ck3?\n",
    "convolutional_neural_networks_a_1_1: [8, 128]\n",
    "\n",
    "# Question 1.2 (/2): What is the dimension of ck4?\n",
    "convolutional_neural_networks_a_1_2: [7, 128]\n",
    "\n",
    "# Question 1.3 (/2): What is the dimension of ck5?\n",
    "convolutional_neural_networks_a_1_3: [6, 128]\n",
    "\n",
    "# Question 1.4 (/2): What is the dimension of chatk3?\n",
    "convolutional_neural_networks_a_1_4: [128, 1]\n",
    "\n",
    "# Question 1.5 (/3): What is the dimension of chatk4?\n",
    "convolutional_neural_networks_a_1_5: [128, 1]\n",
    "\n",
    "# Question 1.6 (/3): What is the dimension of chatk5?\n",
    "convolutional_neural_networks_a_1_6: [128, 1]\n",
    "\n",
    "# Question 1.7 (/3): What is the dimension of Chat?\n",
    "convolutional_neural_networks_a_1_7: [384, 1]\n",
    "\n",
    "# Question 2.1 (/3): What is the dimension of ck3?\n",
    "convolutional_neural_networks_a_2_1: [10, 1]\n",
    "\n",
    "# Question 2.2 (/3): What is the dimension of chatk3?\n",
    "convolutional_neural_networks_a_2_2: [128, 1]\n",
    "\n",
    "# Question 3.1 (/3): How many parameters are there in Wfilter=3?\n",
    "convolutional_neural_networks_a_3_1: 30\n",
    "\n",
    "# Question 3.2 (/3): How many parameters are there in Wfilter=4?\n",
    "convolutional_neural_networks_a_3_2: 40\n",
    "\n",
    "# Question 3.3 (/3): How many parameters are there in Wfilter=5?\n",
    "convolutional_neural_networks_a_3_3: 50\n",
    "\n",
    "# Question 3.4 (/3): How many parameters are there in Wout?\n",
    "convolutional_neural_networks_a_3_4: 2688\n",
    "\n",
    "# Question 4 (/1): Compare kernels to feature engineering.\n",
    "# This question is a candidate for discussion in live session.\n",
    "convolutional_neural_networks_a_4: your answer\n",
    "\n",
    "# Question 5.1 (/2): Would the two predictions be the same?\n",
    "convolutional_neural_networks_a_5_1: False\n",
    "\n",
    "# Question 5.2 (/0): Why or why not?\n",
    "convolutional_neural_networks_a_5_2: These words are close enough in proximity that the filters will see the change.  If 'bar' negates then the sentiment could be reversted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 10, 10)       100         input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 8, 128)       3968        embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 7, 128)       5248        embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 6, 128)       6528        embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_57 (Global (None, 128)          0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_58 (Global (None, 128)          0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_59 (Global (None, 128)          0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 384)          0           global_max_pooling1d_57[0][0]    \n",
      "                                                                 global_max_pooling1d_58[0][0]    \n",
      "                                                                 global_max_pooling1d_59[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 384)          0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 7)            2695        dropout_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 18,539\n",
      "Trainable params: 18,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 1\n",
    "embed_dim = 10\n",
    "num_classes = 7\n",
    "num_filters = [128, 128, 128]\n",
    "kernel_sizes = [3, 4, 5]\n",
    "dense_layer_dims = []\n",
    "dropout_rate = 0.9\n",
    "max_input_length=10\n",
    "\n",
    "\n",
    "wordids = keras.layers.Input(shape=(max_input_length,))\n",
    "\n",
    "h = keras.layers.Embedding(max_input_length, embed_dim, input_length=10)(wordids)\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='VALID')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "print(prediction.shape)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the CNN Model\n",
    "\n",
    "We'll implement our CNN model below. Our implementation will differ from [Kim (2014)](https://www.aclweb.org/anthology/D14-1181) in that we will support using multiple dense hidden layers after the convolutional layers.\n",
    "\n",
    "**Before you start**, be sure to answer the short-answer questions above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Building vocabulary - 16,474 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "\n",
    "# Load SST dataset\n",
    "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")\n",
    "max_len = 40\n",
    "train_x, train_ns, train_y = ds.as_padded_array('train', max_len=max_len, root_only=True)\n",
    "dev_x,   dev_ns,   dev_y   = ds.as_padded_array('dev',   max_len=max_len, root_only=True)\n",
    "test_x,  test_ns,  test_y  = ds.as_padded_array('test',  max_len=max_len, root_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First h: (?, 40, 5)\n",
      "Second h: (?, 6)\n",
      "Third h: (?, 6)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 40, 5)        82370       input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 39, 2)        22          embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 38, 2)        32          embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 37, 2)        42          embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_66 (Global (None, 2)            0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_67 (Global (None, 2)            0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_68 (Global (None, 2)            0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 6)            0           global_max_pooling1d_66[0][0]    \n",
      "                                                                 global_max_pooling1d_67[0][0]    \n",
      "                                                                 global_max_pooling1d_68[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 6)            0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 2)            14          dropout_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 82,480\n",
      "Trainable params: 82,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 10\n",
    "embed_dim = 5\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = []\n",
    "dropout_rate = 0.8\n",
    "num_classes = len(ds.target_names)\n",
    "\n",
    "# Construct the convolutional neural network.\n",
    "# The form of each keras layer function is as follows:\n",
    "#    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "# concretely,\n",
    "#    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "# performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "print(\"First h: {}\".format(h.shape))\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 10 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "print(\"Second h: {}\".format(h.shape))\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "print(\"Third h: {}\".format(h.shape))\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "\n",
    "print(type(h))\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kevinhanna/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/kevinhanna/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 3s 472us/sample - loss: 0.6932 - acc: 0.5163\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 2s 223us/sample - loss: 0.6876 - acc: 0.5477\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 2s 224us/sample - loss: 0.6769 - acc: 0.5818\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 1s 197us/sample - loss: 0.6646 - acc: 0.6107\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 1s 203us/sample - loss: 0.6515 - acc: 0.6225\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 1s 211us/sample - loss: 0.6357 - acc: 0.6368\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 2s 222us/sample - loss: 0.6241 - acc: 0.6516\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 1s 201us/sample - loss: 0.6018 - acc: 0.6705\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 1s 164us/sample - loss: 0.5937 - acc: 0.6727\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 1s 170us/sample - loss: 0.5758 - acc: 0.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd91f28ebe0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "model.fit(train_x, train_y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Call [evaluate](https://keras.io/models/model/#evaluate) on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872/872 [==============================] - 0s 227us/sample - loss: 0.6929 - acc: 0.5092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6929376087057482, 0.5091743]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "model.evaluate(dev_x, dev_y)\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c): Tuning Your Model\n",
    "\n",
    "We'll once again want to optimize hyperparameters for our model to see if we can improve performance. The CNN model includes a number of new parameters that can significantly influence model performance.\n",
    "\n",
    "In this section, you will be asked to describe the new parameters as well as use them to attempt to improve the performance of your model.\n",
    "\n",
    "## Part (c) Short Answer Questions\n",
    "\n",
    "  1. Choose two parameters unique the CNN model, perform at least 10 runs with different combinations of values for these parameters, and then report the dev set results below. ***Hint: Consider wrapping the training code above in a for loop the examines the different values.***  To do this efficiently, you should consider [this paper](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) from Bergstra and Bengio.  [This blog post](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) also has a less formal treatment of the same topic.\n",
    "  2. Describe any trends you see in experiments above (e.g., can you identify good ranges for the individual parameters; are there any interesting interactions?)\n",
    "  3. Pick the three best configurations according to the dev set and evaluate them on the test data. Is the ranking of the three best models the same on the dev and test sets?\n",
    "  4. What was the best accuracy you achieved on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 799us/sample - loss: 0.6912 - acc: 0.5264\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 466us/sample - loss: 0.6353 - acc: 0.6500\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 460us/sample - loss: 0.4238 - acc: 0.8238\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 461us/sample - loss: 0.2551 - acc: 0.9081\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 462us/sample - loss: 0.1605 - acc: 0.9457\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 459us/sample - loss: 0.0985 - acc: 0.9699\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 453us/sample - loss: 0.0685 - acc: 0.9782\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 454us/sample - loss: 0.0512 - acc: 0.9828\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 456us/sample - loss: 0.0405 - acc: 0.9866\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 456us/sample - loss: 0.0326 - acc: 0.9910\n",
      "872/872 [==============================] - 2s 3ms/sample - loss: 1.1140 - acc: 0.7638\n",
      "1821/1821 [==============================] - 0s 208us/sample - loss: 1.0706 - acc: 0.7694\n",
      "Iter 2 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 5s 773us/sample - loss: 0.6933 - acc: 0.5105\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 448us/sample - loss: 0.6903 - acc: 0.5238\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 445us/sample - loss: 0.6581 - acc: 0.6137\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 446us/sample - loss: 0.5281 - acc: 0.7640\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 446us/sample - loss: 0.3889 - acc: 0.8504\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 454us/sample - loss: 0.2925 - acc: 0.8970\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 449us/sample - loss: 0.2079 - acc: 0.9292\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 443us/sample - loss: 0.1625 - acc: 0.9464\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 440us/sample - loss: 0.1224 - acc: 0.9594\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 441us/sample - loss: 0.1011 - acc: 0.9653\n",
      "872/872 [==============================] - 2s 3ms/sample - loss: 0.8254 - acc: 0.7821\n",
      "1821/1821 [==============================] - 0s 230us/sample - loss: 0.7932 - acc: 0.7628\n",
      "Iter 3 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 806us/sample - loss: 0.6921 - acc: 0.5165\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 446us/sample - loss: 0.6878 - acc: 0.5353\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 444us/sample - loss: 0.6533 - acc: 0.6185\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 454us/sample - loss: 0.5493 - acc: 0.7467\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 450us/sample - loss: 0.4424 - acc: 0.8129\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 448us/sample - loss: 0.3499 - acc: 0.8617\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 448us/sample - loss: 0.2708 - acc: 0.8967\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 451us/sample - loss: 0.2119 - acc: 0.9247\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 452us/sample - loss: 0.1748 - acc: 0.9373\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 449us/sample - loss: 0.1441 - acc: 0.9496\n",
      "872/872 [==============================] - 2s 3ms/sample - loss: 0.8907 - acc: 0.7294\n",
      "1821/1821 [==============================] - 0s 210us/sample - loss: 0.8080 - acc: 0.7435\n",
      "Iter 4 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 849us/sample - loss: 0.6926 - acc: 0.5217\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 473us/sample - loss: 0.6806 - acc: 0.5470\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 478us/sample - loss: 0.5859 - acc: 0.7296\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 484us/sample - loss: 0.4292 - acc: 0.8233\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 475us/sample - loss: 0.3107 - acc: 0.8757\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 490us/sample - loss: 0.2180 - acc: 0.9227\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 477us/sample - loss: 0.1673 - acc: 0.9425\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 472us/sample - loss: 0.1263 - acc: 0.9579\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 480us/sample - loss: 0.1020 - acc: 0.9684\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 475us/sample - loss: 0.0875 - acc: 0.9738\n",
      "872/872 [==============================] - 2s 3ms/sample - loss: 0.8976 - acc: 0.7626\n",
      "1821/1821 [==============================] - 0s 197us/sample - loss: 0.8954 - acc: 0.7677\n",
      "Iter 5 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 865us/sample - loss: 0.6915 - acc: 0.5237\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 497us/sample - loss: 0.5898 - acc: 0.6890\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 492us/sample - loss: 0.3784 - acc: 0.8510\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 487us/sample - loss: 0.2347 - acc: 0.9108\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 493us/sample - loss: 0.1418 - acc: 0.9494\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 493us/sample - loss: 0.0913 - acc: 0.9672\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 495us/sample - loss: 0.0642 - acc: 0.9805\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 500us/sample - loss: 0.0497 - acc: 0.9809\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 505us/sample - loss: 0.0434 - acc: 0.9832\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 4s 513us/sample - loss: 0.0324 - acc: 0.9883\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 1.1335 - acc: 0.7695\n",
      "1821/1821 [==============================] - 0s 248us/sample - loss: 1.1014 - acc: 0.7650\n",
      "Iter 6 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 858us/sample - loss: 0.6928 - acc: 0.5159\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 4s 510us/sample - loss: 0.6176 - acc: 0.6555\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 494us/sample - loss: 0.4106 - acc: 0.8211\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 499us/sample - loss: 0.2570 - acc: 0.9000\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 4s 509us/sample - loss: 0.1662 - acc: 0.9393\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 4s 508us/sample - loss: 0.1144 - acc: 0.9605\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 506us/sample - loss: 0.0784 - acc: 0.9707\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 4s 515us/sample - loss: 0.0578 - acc: 0.9801\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 4s 510us/sample - loss: 0.0467 - acc: 0.9840\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 497us/sample - loss: 0.0462 - acc: 0.9835\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 1.1333 - acc: 0.7649\n",
      "1821/1821 [==============================] - 0s 241us/sample - loss: 1.0840 - acc: 0.7595\n",
      "Iter 7 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 903us/sample - loss: 0.6935 - acc: 0.5129\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 491us/sample - loss: 0.6498 - acc: 0.6152\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 506us/sample - loss: 0.4672 - acc: 0.7866\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 502us/sample - loss: 0.2908 - acc: 0.8831\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 499us/sample - loss: 0.1796 - acc: 0.9315\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 504us/sample - loss: 0.1242 - acc: 0.9561\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 501us/sample - loss: 0.0843 - acc: 0.9695\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 501us/sample - loss: 0.0602 - acc: 0.9803\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 495us/sample - loss: 0.0532 - acc: 0.9811\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920/6920 [==============================] - 3s 499us/sample - loss: 0.0415 - acc: 0.9864\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 1.1515 - acc: 0.7603\n",
      "1821/1821 [==============================] - 0s 245us/sample - loss: 1.1014 - acc: 0.7666\n",
      "Iter 8 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 830us/sample - loss: 0.6931 - acc: 0.5152\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 461us/sample - loss: 0.6893 - acc: 0.5212\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 460us/sample - loss: 0.6524 - acc: 0.6276\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 462us/sample - loss: 0.5100 - acc: 0.7763\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 464us/sample - loss: 0.3577 - acc: 0.8646\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 473us/sample - loss: 0.2461 - acc: 0.9114\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 462us/sample - loss: 0.1904 - acc: 0.9363\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 460us/sample - loss: 0.1347 - acc: 0.9558\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 458us/sample - loss: 0.0983 - acc: 0.9662\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 459us/sample - loss: 0.0838 - acc: 0.9725\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 0.8962 - acc: 0.7661\n",
      "1821/1821 [==============================] - 0s 158us/sample - loss: 0.9123 - acc: 0.7694\n",
      "Iter 9 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 934us/sample - loss: 0.6916 - acc: 0.5129\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 493us/sample - loss: 0.6188 - acc: 0.6629\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 4s 511us/sample - loss: 0.4696 - acc: 0.7860\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 4s 507us/sample - loss: 0.3607 - acc: 0.8510\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 500us/sample - loss: 0.2786 - acc: 0.8899\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 4s 506us/sample - loss: 0.2197 - acc: 0.9133\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 492us/sample - loss: 0.1842 - acc: 0.9288\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 496us/sample - loss: 0.1489 - acc: 0.9449\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 4s 508us/sample - loss: 0.1287 - acc: 0.9535\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 4s 520us/sample - loss: 0.1127 - acc: 0.9572\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 1.0490 - acc: 0.6950\n",
      "1821/1821 [==============================] - 0s 241us/sample - loss: 1.0485 - acc: 0.7024\n",
      "Iter 10 of 10\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 6s 884us/sample - loss: 0.6931 - acc: 0.5179\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 3s 462us/sample - loss: 0.6889 - acc: 0.5262\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 3s 471us/sample - loss: 0.6602 - acc: 0.6085\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 3s 467us/sample - loss: 0.5606 - acc: 0.7487\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 3s 458us/sample - loss: 0.4483 - acc: 0.8059\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 3s 468us/sample - loss: 0.3522 - acc: 0.8514\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 3s 457us/sample - loss: 0.2797 - acc: 0.8939\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 3s 457us/sample - loss: 0.2268 - acc: 0.9143\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 3s 462us/sample - loss: 0.1772 - acc: 0.9367\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 3s 471us/sample - loss: 0.1423 - acc: 0.9496\n",
      "872/872 [==============================] - 3s 3ms/sample - loss: 0.9365 - acc: 0.7259\n",
      "1821/1821 [==============================] - 0s 242us/sample - loss: 0.9039 - acc: 0.7381\n",
      "[{'accuracy_dev': 0.76376146, 'accuracy_test': 0.7693575, 'loss': 1.1139778947338053, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 4, 6], 'dense_layer_dims': [10]}, {'accuracy_dev': 0.7821101, 'accuracy_test': 0.76276773, 'loss': 0.8253861922736562, 'dropout_rate': 0.9, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 3, 4], 'dense_layer_dims': [10]}, {'accuracy_dev': 0.7293578, 'accuracy_test': 0.7435475, 'loss': 0.8906924483972952, 'dropout_rate': 0.9, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 3, 4], 'dense_layer_dims': [10]}, {'accuracy_dev': 0.76261467, 'accuracy_test': 0.76771003, 'loss': 0.8975968937808221, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 4, 6], 'dense_layer_dims': [10, 20, 30, 40]}, {'accuracy_dev': 0.7694954, 'accuracy_test': 0.7649643, 'loss': 1.1334658850223647, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 3, 4], 'dense_layer_dims': [400]}, {'accuracy_dev': 0.76490825, 'accuracy_test': 0.7594728, 'loss': 1.1332963661316338, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 4, 6], 'dense_layer_dims': [400]}, {'accuracy_dev': 0.7603211, 'accuracy_test': 0.76661175, 'loss': 1.1515325676957402, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 4, 6], 'dense_layer_dims': [400]}, {'accuracy_dev': 0.76605505, 'accuracy_test': 0.7693575, 'loss': 0.8961545557057092, 'dropout_rate': 0.9, 'num_filters': [40, 50, 60], 'kernel_sizes': [3, 4, 5], 'dense_layer_dims': [10]}, {'accuracy_dev': 0.69495416, 'accuracy_test': 0.70236135, 'loss': 1.04898545501429, 'dropout_rate': 0.8, 'num_filters': [40, 50, 60], 'kernel_sizes': [2, 3, 4], 'dense_layer_dims': [400]}, {'accuracy_dev': 0.72591746, 'accuracy_test': 0.738056, 'loss': 0.9365277252066027, 'dropout_rate': 0.9, 'num_filters': [40, 50, 60], 'kernel_sizes': [3, 4, 5], 'dense_layer_dims': [10]}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import random\n",
    "\n",
    "# Specify model hyperparameters.\n",
    "\n",
    "def create_model(\n",
    "    epochs = 10,\n",
    "    embed_dim = 5,\n",
    "    num_filters = [2, 2, 2],\n",
    "    kernel_sizes = [2, 3, 4],\n",
    "    dense_layer_dims = [],\n",
    "    dropout_rate = 0.8\n",
    "):\n",
    "    num_classes = len(ds.target_names)\n",
    "    # Construct the convolutional neural network.\n",
    "    # The form of each keras layer function is as follows:\n",
    "    #    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "    # concretely,\n",
    "    #    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "    # performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "    # Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "    # In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "    wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "    # Embed the wordids.\n",
    "    # Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "    h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "    # Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "    # With the default hyperparameters, we construct 10 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "    # is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "    # function name below).\n",
    "    conv_layers_for_all_kernel_sizes = []\n",
    "    for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "    # Concat the feature maps from each different size.\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "    # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "    # in the vector.\n",
    "    # See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "    \n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "\n",
    "    for dense_layer_dim in dense_layer_dims:\n",
    "        h = keras.layers.Dense(dense_layer_dim,\n",
    "                  use_bias=True,\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='glorot_normal',\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_regularizer=None,\n",
    "                  name='Dense_Encoder_' + str(dense_layer_dim))(h)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "    model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "                  metrics=['accuracy'])        # What metric to output as we train.\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_score(model, epochs=10):\n",
    "    model.reset_states()\n",
    "    model.fit(train_x, train_y, epochs=epochs)\n",
    "#     print(model.summary())\n",
    "    dev_score = model.evaluate(dev_x, dev_y)\n",
    "    test_score = model.evaluate(test_x, test_y)\n",
    "    \n",
    "    return dev_score, test_score\n",
    "\n",
    "param_dist = dict(    \n",
    "    dropout_rate=[0.80, 0.9, 0.95],\n",
    "    num_filters=[[10, 20, 30], [40, 50, 60], [20, 20, 20]],\n",
    "    kernel_sizes=[[2, 3, 4], [2, 4, 6], [3, 4, 5]],\n",
    "    dense_layer_dims=[[10, 20, 30, 40], [10], [400]]\n",
    ")\n",
    "\n",
    "\"\"\" Random Search \"\"\"\n",
    "n_iter_search = 10\n",
    "num_epochs = 10\n",
    "# random_model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model())\n",
    "# random_search = RandomizedSearchCV(estimator=random_model, \n",
    "#                                    param_distributions=param_dist,\n",
    "#                                    n_iter=n_iter_search,\n",
    "#                                    n_jobs=1,\n",
    "#                                    cv=2,\n",
    "#                                    verbose=5,\n",
    "#                                    scoring=\"accuracy\"\n",
    "#                                   )\n",
    "# random_search.fit(train_x, train_y)\n",
    "\n",
    "# # Show the results\n",
    "# print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "# means = random_search.cv_results_['mean_test_score']\n",
    "# stds = random_search.cv_results_['std_test_score']\n",
    "# params = random_search.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "def random_search():\n",
    "    results = []\n",
    "    for n in range(n_iter_search):\n",
    "        print(\"Iter {} of {}\".format(n+1, n_iter_search))\n",
    "        dropout_rate = param_dist['dropout_rate'][random.randrange(len(param_dist['dropout_rate']))]\n",
    "        kernel_sizes = param_dist['kernel_sizes'][random.randrange(len(param_dist['kernel_sizes']))]\n",
    "        dense_layer_dims = param_dist['dense_layer_dims'][random.randrange(len(param_dist['dense_layer_dims']))]\n",
    "        num_filters = param_dist['num_filters'][1] #[random.randrange(len(param_dist['num_filters']))]\n",
    "        \n",
    "        model = create_model(epochs=num_epochs,\n",
    "                             dropout_rate=dropout_rate, \n",
    "                             num_filters=num_filters, \n",
    "                             kernel_sizes=kernel_sizes, \n",
    "                             dense_layer_dims=dense_layer_dims,\n",
    "                             )\n",
    "        \n",
    "        dev_score, test_score = train_and_score(model, epochs=num_epochs)\n",
    "        \n",
    "        results.append({\"accuracy_dev\": dev_score[1],\n",
    "                        \"accuracy_test\": test_score[1],\n",
    "                        \"loss\": dev_score[0],\n",
    "                        \"dropout_rate\": dropout_rate, \n",
    "                        \"num_filters\": num_filters, \n",
    "                        \"kernel_sizes\": kernel_sizes, \n",
    "                        \"dense_layer_dims\": dense_layer_dims})\n",
    "        \n",
    "    return results\n",
    "    \n",
    "\"\"\" Grid Search \"\"\"\n",
    "def grid_search():\n",
    "    dropout_rates=[0.82]\n",
    "    num_filterses=[[120, 180, 240, 300, 360]]\n",
    "    kernel_sizeses=[[2, 3, 4, 5, 6], [2, 4, 6, 8, 10]]\n",
    "    denses=[[300, 400]]\n",
    "\n",
    "    results = []\n",
    "    for dr in dropout_rates:\n",
    "        for nf in num_filterses:\n",
    "            for ks in kernel_sizeses:\n",
    "                for dense in denses:\n",
    "                    model = create_model(dropout_rate=dr, num_filters=nf, kernel_sizes=ks, dense_layer_dims=dense)\n",
    "                    dev_score, test_score = train_and_score(model)\n",
    "                    results.append({\"accuracy_dev\": dev_score[1],\n",
    "                                    \"accuracy_test\": test_score[1], \n",
    "                                    \"loss\": dev_score[0], \n",
    "                                    \"dropout_rate\": dr, \n",
    "                                    \"num_filters\": nf, \n",
    "                                    \"kernel_sizes\": ks, \n",
    "                                    \"dense_layer_dims\": dense})\n",
    "    return results\n",
    "\n",
    "\n",
    "random_search_results = random_search()\n",
    "print(random_search_results)\n",
    "\n",
    "# grid_search_results = grid_search()\n",
    "# print(grid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.7821100950241089 accuracy_test: 0.7627677321434021 dropout rate: 0.9  number of filters: [40, 50, 60] kernel sizes: [2, 3, 4] dense: [10]\n",
      "\n",
      "accuracy: 0.7821100950241089 accuracy_test: 0.7627677321434021  dropout rate: 0.9  number of filters: [40, 50, 60] kernel sizes: [2, 3, 4] dense: [10]\n",
      "accuracy: 0.76949542760849 accuracy_test: 0.7649642825126648  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 3, 4] dense: [400]\n",
      "accuracy: 0.7660550475120544 accuracy_test: 0.7693575024604797  dropout rate: 0.9  number of filters: [40, 50, 60] kernel sizes: [3, 4, 5] dense: [10]\n",
      "accuracy: 0.7649082541465759 accuracy_test: 0.7594727873802185  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 4, 6] dense: [400]\n",
      "accuracy: 0.7637614607810974 accuracy_test: 0.7693575024604797  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 4, 6] dense: [10]\n",
      "accuracy: 0.7626146674156189 accuracy_test: 0.7677100300788879  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 4, 6] dense: [10, 20, 30, 40]\n",
      "accuracy: 0.7603210806846619 accuracy_test: 0.7666117548942566  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 4, 6] dense: [400]\n",
      "accuracy: 0.7293577790260315 accuracy_test: 0.7435474991798401  dropout rate: 0.9  number of filters: [40, 50, 60] kernel sizes: [2, 3, 4] dense: [10]\n",
      "accuracy: 0.7259174585342407 accuracy_test: 0.7380560040473938  dropout rate: 0.9  number of filters: [40, 50, 60] kernel sizes: [3, 4, 5] dense: [10]\n",
      "accuracy: 0.6949541568756104 accuracy_test: 0.7023613452911377  dropout rate: 0.8  number of filters: [40, 50, 60] kernel sizes: [2, 3, 4] dense: [400]\n"
     ]
    }
   ],
   "source": [
    "sorted_results = sorted(random_search_results, key=lambda k: k['accuracy_dev'], reverse=True) \n",
    "\n",
    "top_res=sorted_results[0]\n",
    "\n",
    "print(\"Best accuracy: {} accuracy_test: {} dropout rate: {}  number of filters: {} kernel sizes: {} dense: {}\\n\".format(\n",
    "    top_res['accuracy_dev'], \n",
    "    top_res['accuracy_test'], \n",
    "    top_res['dropout_rate'], \n",
    "    top_res['num_filters'], \n",
    "    top_res['kernel_sizes'], \n",
    "    top_res['dense_layer_dims']))\n",
    "\n",
    "for result in sorted_results:\n",
    "    print(\"accuracy: {} accuracy_test: {}  dropout rate: {}  number of filters: {} kernel sizes: {} dense: {}\".format(\n",
    "         result['accuracy_dev'], result['accuracy_test'], result['dropout_rate'], result['num_filters'], result['kernel_sizes'], result['dense_layer_dims']))\n",
    "#     print(result)\n",
    "\n",
    "# Best accuracy: 0.7775229215621948 dropout rate: 0.85  number of filters: [240, 240, 240, 240] kernel sizes: [2, 4, 6, 8] dense: [300, 400]\n",
    "# Best accuracy: 0.7798165082931519 dropout rate: 0.82  number of filters: [120, 180, 240, 300, 360] kernel sizes: [2, 3, 4, 5, 6] dense: [300, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
